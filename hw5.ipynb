{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# ПАТЧИНГ БИБЛИОТЕКИ\n",
        "\n",
        "try:\n",
        "    import tam\n",
        "    from tam import TAM\n",
        "except ImportError:\n",
        "    print(\"tam.py не найден.\")\n",
        "    exit(1)\n",
        "\n",
        "_original_id2idx = tam.id2idx\n",
        "\n",
        "def patched_id2idx(inp_id, target_id, return_last=False):\n",
        "    if isinstance(target_id, tuple) and target_id[0] == 'force_index':\n",
        "        return target_id[1]\n",
        "    return _original_id2idx(inp_id, target_id, return_last)\n",
        "\n",
        "tam.id2idx = patched_id2idx\n",
        "print(\"Библиотека TAM успешно пропатчена для работы с Qwen2-VL.\")\n",
        "\n",
        "MODEL_PATH = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "IMAGE_PATH = \"demo_image.jpg\"\n",
        "PROMPT_TEXT = \"Describe this image in detail.\"\n",
        "SAVE_DIR = \"results_hw5\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Загрузка модели {MODEL_PATH}...\")\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_PATH, torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32, device_map=\"auto\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
        "\n",
        "if not os.path.exists(IMAGE_PATH):\n",
        "    import requests\n",
        "    url = \"https://raw.githubusercontent.com/QwenLM/Qwen-VL/master/assets/demo.jpeg\"\n",
        "    Image.open(requests.get(url, stream=True).raw).save(IMAGE_PATH)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": IMAGE_PATH},\n",
        "            {\"type\": \"text\", \"text\": PROMPT_TEXT},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "image_inputs, video_inputs = process_vision_info(messages)\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    videos=video_inputs,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(DEVICE)\n",
        "\n",
        "print(\"Генерация ответа...\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=40,\n",
        "        use_cache=True,\n",
        "        output_hidden_states=True,\n",
        "        return_dict_in_generate=True\n",
        "    )\n",
        "\n",
        "generated_ids = outputs.sequences[0]\n",
        "input_len = inputs.input_ids.shape[1]\n",
        "new_tokens = generated_ids[input_len:]\n",
        "\n",
        "print(f\"Ответ: {processor.decode(new_tokens, skip_special_tokens=True)}\")\n",
        "\n",
        "logits_list = []\n",
        "logits_list.append(model.lm_head(outputs.hidden_states[0][-1]))\n",
        "for state in outputs.hidden_states[1:]:\n",
        "    logits_list.append(model.lm_head(state[-1]))\n",
        "\n",
        "if 'image_grid_thw' in inputs:\n",
        "    thw = inputs['image_grid_thw'][0]\n",
        "    vision_shape = (int(thw[1] // 2), int(thw[2] // 2))\n",
        "    print(f\"Calculated Vision Shape: {vision_shape} (tokens), Original grid: {thw[1]}x{thw[2]}\")\n",
        "else:\n",
        "    vision_shape = (24, 24)\n",
        "\n",
        "vis_start_id = processor.tokenizer.convert_tokens_to_ids(\"<|vision_start|>\")\n",
        "vis_end_id = processor.tokenizer.convert_tokens_to_ids(\"<|vision_end|>\")\n",
        "\n",
        "special_ids = {\n",
        "    'img_id': [vis_start_id, vis_end_id], # Тут оставляем поиск по токенам, это работает\n",
        "    'prompt_id': [('force_index', 0), ('force_index', input_len - 1)],\n",
        "    'answer_id': [('force_index', input_len), ('force_index', len(generated_ids) - 1)]\n",
        "}\n",
        "\n",
        "vis_inputs = [Image.open(IMAGE_PATH).convert(\"RGB\")]\n",
        "raw_map_records = []\n",
        "\n",
        "print(f\"Запуск визуализации в папку {SAVE_DIR}...\")\n",
        "\n",
        "for i, token_id in enumerate(new_tokens):\n",
        "    token_str = processor.decode([token_id])\n",
        "    safe_name = \"\".join([c if c.isalnum() else \"_\" for c in token_str]).strip()\n",
        "    if not safe_name: safe_name = \"sym\"\n",
        "\n",
        "    filename = os.path.join(SAVE_DIR, f\"token_{i:02d}_{safe_name}.jpg\")\n",
        "    print(f\"[{i}] {token_str} -> {filename}\")\n",
        "\n",
        "    try:\n",
        "        TAM(\n",
        "            generated_ids.cpu().tolist(),\n",
        "            vision_shape,\n",
        "            logits_list,\n",
        "            special_ids,\n",
        "            vis_inputs,\n",
        "            processor,\n",
        "            filename,\n",
        "            i,\n",
        "            raw_map_records,\n",
        "            False\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR on token {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"Завершено.\")"
      ],
      "metadata": {
        "id": "BFyDlPSmK2m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ОТЧЕТ ПО ДЗ 5\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nРЕЗУЛЬТАТ ЭКСПЕРИМЕНТА:\")\n",
        "print(\" Показал неудовлетворительные результаты.\")\n",
        "\n",
        "print(\"\\nОСНОВНЫЕ ПРОБЛЕМЫ:\")\n",
        "print(\"1. [Качество визуализации]: Тепловые карты (activation maps) сильно зашумлены.\")\n",
        "print(\"   Области внимания (красные пятна) не совпадают с генерируемыми объектами\")\n",
        "print(\"   (например, подсветка фона вместо объекта или хаотичные пятна).\")\n",
        "\n",
        "print(\"2. [Техническая реализация]: Метод TAM оказался несовместим 'из коробки'\")\n",
        "print(\"   с архитектурой Qwen2-VL. Модель использует динамическое разрешение\")\n",
        "print(\"   и объединение патчей (patch merging), из-за чего возникают критические\")\n",
        "print(\"   ошибки размерности тензоров (Shape Mismatch), требующие ручного хардкодинга.\")\n",
        "\n",
        "print(\"\\nВЫВОД:\")\n",
        "print(\"Применение метода TAM не выдало ожидаемого ответа\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQQJZfD9MqQo",
        "outputId": "e4a3da4f-8b25-433d-9806-4373c9c90265"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ОТЧЕТ ПО ДЗ 5\n",
            "============================================================\n",
            "\n",
            "РЕЗУЛЬТАТ ЭКСПЕРИМЕНТА:\n",
            " Показал неудовлетворительные результаты.\n",
            "\n",
            "ОСНОВНЫЕ ПРОБЛЕМЫ:\n",
            "1. [Качество визуализации]: Тепловые карты (activation maps) сильно зашумлены.\n",
            "   Области внимания (красные пятна) не совпадают с генерируемыми объектами\n",
            "   (например, подсветка фона вместо объекта или хаотичные пятна).\n",
            "2. [Техническая реализация]: Метод TAM оказался несовместим 'из коробки'\n",
            "   с архитектурой Qwen2-VL. Модель использует динамическое разрешение\n",
            "   и объединение патчей (patch merging), из-за чего возникают критические\n",
            "   ошибки размерности тензоров (Shape Mismatch), требующие ручного хардкодинга.\n",
            "\n",
            "ВЫВОД:\n",
            "Применение метода TAM не выдало ожидаемого ответа\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_dVPNKeTCUu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}